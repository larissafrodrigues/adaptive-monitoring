{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "packetvision.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "120f5be2f7f648fb9cf67555394fc929": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_18a77557111d4ad2bec5d80ba2961d05",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a2603e8c81f74a1cb00c357452549d73",
              "IPY_MODEL_fa4ca5fce4e7476fb39ce35687d0bdf2",
              "IPY_MODEL_15d81195d273415a99dba0d9f8ce3763"
            ]
          }
        },
        "18a77557111d4ad2bec5d80ba2961d05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a2603e8c81f74a1cb00c357452549d73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e5b4d68efef3473a8e4689ae37f8f94a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0a26e0dcbc954f96b16d53008fcaf7ef"
          }
        },
        "fa4ca5fce4e7476fb39ce35687d0bdf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3d97da8b8f2c4b979c6c8e120d14e777",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 5010551,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5010551,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6c5efbcbc4b84a6b993c580dc9465b07"
          }
        },
        "15d81195d273415a99dba0d9f8ce3763": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e52089bc076d4baeaa239a99ffa807d7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4.78M/4.78M [00:00&lt;00:00, 12.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7e1d821aa4f648798a832bd0e21ac9a6"
          }
        },
        "e5b4d68efef3473a8e4689ae37f8f94a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0a26e0dcbc954f96b16d53008fcaf7ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3d97da8b8f2c4b979c6c8e120d14e777": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6c5efbcbc4b84a6b993c580dc9465b07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e52089bc076d4baeaa239a99ffa807d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7e1d821aa4f648798a832bd0e21ac9a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VmYCv5MSvmV",
        "outputId": "781c8a74-2fc7-4cd4-9a1b-8bfafc666d5d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2Py-Aikk6P3",
        "outputId": "beef908f-948c-4305-b799-27aa4376ae42"
      },
      "source": [
        "cd gdrive/MyDrive/Dev/VINEVI/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Dev/VINEVI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "120f5be2f7f648fb9cf67555394fc929",
            "18a77557111d4ad2bec5d80ba2961d05",
            "a2603e8c81f74a1cb00c357452549d73",
            "fa4ca5fce4e7476fb39ce35687d0bdf2",
            "15d81195d273415a99dba0d9f8ce3763",
            "e5b4d68efef3473a8e4689ae37f8f94a",
            "0a26e0dcbc954f96b16d53008fcaf7ef",
            "3d97da8b8f2c4b979c6c8e120d14e777",
            "6c5efbcbc4b84a6b993c580dc9465b07",
            "e52089bc076d4baeaa239a99ffa807d7",
            "7e1d821aa4f648798a832bd0e21ac9a6"
          ]
        },
        "id": "I3D1um8ITlCH",
        "outputId": "17cf8157-beac-4e7e-d61b-17250d07446e"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Mar  9 19:32:42 2021\n",
        "\n",
        "@author: larissa\n",
        "\"\"\"\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Feb 15 21:09:22 2021\n",
        "\n",
        "@author: larissa\n",
        "\"\"\"\n",
        "'''\n",
        "Corrigir o erro ao ler TIF: \n",
        "    conda install libtiff=4.1.0=h885aae3_4 -c conda-forge\n",
        "    \n",
        "    pip install libtiff==0.4.1\n",
        "'''\n",
        "\n",
        "# Importa bibliotecas necessárias\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, models, transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "import time\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "data_dir = '/content/gdrive/MyDrive/Dev/VINEVI/dataset/'\n",
        "\n",
        "classes = ['bittorrent', 'browsing', 'dns', 'iot', 'rdp', 'ssh', 'voip']\n",
        "\n",
        "# Escolhe o modelo: resnet, alexnet, vgg...\n",
        "model_name = \"squeezenet\"\n",
        "\n",
        "# Número de classes do conjunto de dados: [Abnormal, Normal]\n",
        "num_classes = 7\n",
        "# Tamanho do lote para treinamento\n",
        "batch_size = 32\n",
        "num_workers = 2\n",
        "\n",
        "# Número de épocas para treino \n",
        "num_epochs = 50\n",
        "\n",
        "# Extrator de recursos. (False: ajuste do modelo inteiro; True: atualiza apenas os parâmetros da camada remodelada)\n",
        "feature_extract = True\n",
        "\n",
        "# Optimizer e Criterion\n",
        "#lr = 0.001\n",
        "#momentum = 0.9\n",
        "lr = 0.001\n",
        "mmt=0.9\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# Definindo as transformações para o conjunto de treino e validação.\n",
        "# Definindo uma transformação para pré-processar as imagens de treinamento.\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "                           transforms.Resize(size=[224, 224]),\n",
        "                           # Nanni\n",
        "                           #transforms.RandomHorizontalFlip(0.5),\n",
        "                           transforms.RandomVerticalFlip(0.5),\n",
        "                           #transforms.RandomAffine(degrees=0),\n",
        "                           transforms.RandomRotation(30),\n",
        "\n",
        "                           #transforms.Resize(size=[299,299]),\n",
        "                           #transforms.RandomRotation([0,360]),\n",
        "                           #transforms.RandomRotation(30), # melhor densenet\n",
        "                           #transforms.RandomVerticalFlip(0.5), # melhor densenet\n",
        "                           #transforms.RandomHorizontalFlip(0.5),\n",
        "                           #transforms.RandomCrop(224, padding = 10),\n",
        "                           #transforms.GaussianBlur(1.5),\n",
        "                           #transforms.ColorJitter(brightness=1.2, contrast=1.2, saturation=0, hue=0),\n",
        "                           #transforms.RandomGrayscale(p=0.1), # melhor densenet\n",
        "                           #transforms.Grayscale(num_output_channels=3),\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n",
        "                       ])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "                           transforms.Resize(size=[224, 224]),\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n",
        "                       ])\n",
        "\n",
        "trains_dir = []\n",
        "valids_dir = []\n",
        "train_loaders = []\n",
        "valid_loaders = []\n",
        "test_loaders = []\n",
        "\n",
        "folds = os.listdir(data_dir + '5-fold')\n",
        "folds.sort()\n",
        "\n",
        "all_size_train = []\n",
        "all_size_valid = []\n",
        "\n",
        "if __name__ == \"__main__\": \n",
        "\n",
        "    for i in folds:\n",
        "      train_dir = os.path.join(data_dir + '5-fold/', i + '/train/')\n",
        "      valid_dir = os.path.join(data_dir + '5-fold/', i + '/val/')\n",
        "      test_dir = os.path.join(data_dir + '5-fold/', i + '/test/')\n",
        "\n",
        "      print(train_dir)\n",
        "      print(test_dir)\n",
        "    \n",
        "      train_data = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
        "      valid_data = datasets.ImageFolder(valid_dir, transform=test_transforms)\n",
        "      test_data = datasets.ImageFolder(test_dir, transform=test_transforms)\n",
        "    \n",
        "      train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
        "                                                 shuffle=True, num_workers=num_workers)\n",
        "    \n",
        "      valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, \n",
        "                                                 shuffle=True, num_workers=num_workers)\n",
        "    \n",
        "      test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
        "                                                 shuffle=False, num_workers=num_workers)\n",
        "    \n",
        "      train_loaders.append(train_loader)\n",
        "      valid_loaders.append(valid_loader)\n",
        "      test_loaders.append(test_loader)\n",
        "\n",
        "      print(train_loader)\n",
        "      print(test_loader)\n",
        "      \n",
        "    \n",
        "      print(\"----------------------------------------------------------------------------------------\")\n",
        "      print(i)\n",
        "      print('Num training images: ', len(train_data), train_loader)\n",
        "      print('Num valid images: ', len(valid_data), valid_loader)\n",
        "      print('Num test images: ', len(test_data), (test_loader))\n",
        "      \n",
        "      all_size_train.append(len(train_data))\n",
        "      all_size_valid.append(len(valid_data))\n",
        "    \n",
        "    print(\"----------------------------------------------------------------------------------------\")\n",
        "    print(\"\\n\\n----------------------------------------------------------------------------------------\")\n",
        "    print(\"Num train full size:\", sum(all_size_train))\n",
        "    print(\"Num valid full size:\", sum(all_size_valid))\n",
        "    print('Num test images: ', len(test_data), (test_loader))\n",
        "    print(\"Num full size (train+valid):\", sum(all_size_train)+sum(all_size_valid) + len(valid_data))\n",
        "    \n",
        "    \n",
        "    #list = [ x[0] for x in iter(test_loader).next() ]\n",
        "    #print(list)\n",
        "    '''\n",
        "    for xb, yb in test_loader:\n",
        "        print(xb.shape)\n",
        "        #x = xb.view(28,28) \n",
        "        #print(x.shape)\n",
        "        print(xb)\n",
        "        break #just once\n",
        "    '''\n",
        "        \n",
        "    batch = next(iter(test_loader))\n",
        "    print('len:', len(batch))\n",
        "    images, labels = batch\n",
        "    print('types:', type(images), type(labels))\n",
        "    print('shapes:', images.shape, labels.shape)\n",
        "    print(images[0].shape)\n",
        "    print(labels[0])\n",
        "    \n",
        "    grid = torchvision.utils.make_grid(images, nrow=10)\n",
        "    plt.figure(figsize=(15,15))\n",
        "    #plt.imshow(np.transpose(grid, (1,2,0)))\n",
        "    plt.imshow(grid.permute(1,2,0))\n",
        "    print('labels:', labels)\n",
        "\n",
        "    \n",
        "    SEED = 1234\n",
        "    \n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    \n",
        "    GPUavailable = torch.cuda.is_available()\n",
        "    if GPUavailable:\n",
        "        print('Treinamento em GPU!')\n",
        "        device = torch.device(\"cuda:0\")\n",
        "    else:\n",
        "        print('Treinamento em CPU!')\n",
        "        device = \"cpu\"\n",
        "    \n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    \n",
        "\n",
        "    \n",
        "    # Carregando uma rede pré-treinada, treinando e computando a acurácia de validação para cada época\n",
        "    \n",
        "    '''\n",
        "    This helper function sets the .requires_grad\n",
        "    attribute of the parameters in the model to False when we are feature extracting. \n",
        "    By default, when we load a pretrained model all of the parameters have .requires_grad=True, \n",
        "    which is fine if we are training from scratch or finetuning. \n",
        "    However, if we are feature extracting and only want to compute gradients \n",
        "    for the newly initialized layer then we want all of the other parameters \n",
        "    to not require gradients. This will make more sense later.\n",
        "    '''\n",
        "    def set_parameter_requires_grad(model, feature_extracting):\n",
        "        if feature_extracting:\n",
        "            for param in model.parameters():\n",
        "                param.requires_grad = True\n",
        "    \n",
        "    def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "        # Inicializando cada variável específica para cada modelo\n",
        "        model_ft = None\n",
        "        input_size = 0\n",
        "    \n",
        "        if model_name == \"resnet\":\n",
        "            \"\"\" Resnet18\n",
        "            \"\"\"\n",
        "            model_ft = models.resnet18(pretrained=use_pretrained)\n",
        "            set_parameter_requires_grad(model_ft, feature_extract)\n",
        "            num_ftrs = model_ft.fc.in_features\n",
        "            model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "            input_size = 224\n",
        "\n",
        "        if model_name == \"resnext101_32x8d\":\n",
        "            \"\"\" resnext101_32x8d\n",
        "            \"\"\"\n",
        "            model_ft = torch.hub.load('pytorch/vision:v0.9.0', 'resnext101_32x8d', pretrained=use_pretrained)\n",
        "            set_parameter_requires_grad(model_ft, feature_extract)\n",
        "            num_ftrs = model_ft.fc.in_features\n",
        "            model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "            input_size = 224\n",
        "    \n",
        "        elif model_name == \"alexnet\":\n",
        "            \"\"\" Alexnet\n",
        "            \"\"\"\n",
        "            model_ft = models.alexnet(pretrained=use_pretrained)\n",
        "            set_parameter_requires_grad(model_ft, feature_extract)\n",
        "            num_ftrs = model_ft.classifier[6].in_features\n",
        "            model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "            input_size = 224\n",
        "    \n",
        "        elif model_name == \"vgg\":\n",
        "            \"\"\" VGG11_bn\n",
        "            \"\"\"\n",
        "            model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
        "            set_parameter_requires_grad(model_ft, feature_extract)\n",
        "            num_ftrs = model_ft.classifier[6].in_features\n",
        "            model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "            input_size = 224\n",
        "    \n",
        "        elif model_name == \"densenet\":\n",
        "            \"\"\" Densenet\n",
        "            \"\"\"\n",
        "            model_ft = models.densenet169(pretrained=use_pretrained)\n",
        "            #model_ft = models.densenet201(pretrained=use_pretrained)\n",
        "            set_parameter_requires_grad(model_ft, feature_extract)\n",
        "            num_ftrs = model_ft.classifier.in_features\n",
        "            model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n",
        "            input_size = 224\n",
        "    \n",
        "        elif model_name == \"squeezenet\":\n",
        "            model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
        "            set_parameter_requires_grad(model_ft, feature_extract)\n",
        "            model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "            model_ft.num_classes = num_classes\n",
        "            input_size = 224\n",
        "\n",
        "        elif model_name == \"mobilenet\":\n",
        "            model_ft = models.mobilenet_v2(pretrained=use_pretrained)\n",
        "            set_parameter_requires_grad(model_ft, feature_extract)\n",
        "            num_ftrs = model_ft.classifier[1].in_features\n",
        "            model_ft.classifier[1] = nn.Linear(num_ftrs,num_classes)\n",
        "            input_size = 224\n",
        "\n",
        "        elif model_name == \"shufflenet\":\n",
        "            model_ft = models.shufflenet_v2_x1_0(pretrained=True)\n",
        "            set_parameter_requires_grad(model_ft, feature_extract)\n",
        "            num_ftrs = model_ft.fc.in_features\n",
        "            model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "            input_size = 224            \n",
        "            \n",
        "        elif model_name == \"inception\":\n",
        "            \"\"\" Inception v3 \n",
        "            Be careful, expects (299,299) sized images and has auxiliary output\n",
        "            \"\"\"\n",
        "            model_ft = models.inception_v3(pretrained=use_pretrained, aux_logits = False)\n",
        "            set_parameter_requires_grad(model_ft, feature_extract)\n",
        "            # Handle the auxilary net\n",
        "            num_ftrs = model_ft.AuxLogits.fc.in_features\n",
        "            model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
        "            # Handle the primary net\n",
        "            num_ftrs = model_ft.fc.in_features\n",
        "            model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
        "            input_size = 299\n",
        "    \n",
        "        else:\n",
        "            print(\"Invalid model name, exiting...\")\n",
        "            exit()\n",
        "        \n",
        "        return model_ft, input_size\n",
        "    \n",
        "    # Inicializa o modelo para esta execução\n",
        "    model, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "    \n",
        "    # Printa o modelo instanciado\n",
        "    #print(model)   \n",
        "    \n",
        "    # Envia o modelo para a GPU\n",
        "    model = model.to(device)\n",
        "    \n",
        "    # Reúne os parâmetros a serem otimizados/atualizados nesta execução.\n",
        "    # Caso ajustado, estaremos atualizando todos os parâmetros.\n",
        "    # Caso, usando método de extração de recursos, atualiza apenas os parâmetros que acabamos de inicializar,\n",
        "    # ou seja, os parâmetros com require_grad são \"True\".\n",
        "    \n",
        "    params_to_update = model.parameters()\n",
        "    print(\"Params to learn:\")\n",
        "    if feature_extract:\n",
        "        params_to_update = []\n",
        "        for name,param in model.named_parameters():\n",
        "            if param.requires_grad == True:\n",
        "                params_to_update.append(param)\n",
        "                print(\"\\t\",name)\n",
        "    else:\n",
        "        for name,param in model.named_parameters():\n",
        "            if param.requires_grad == True:\n",
        "                print(\"\\t\",name)\n",
        "    \n",
        "    # Optimizer\n",
        "    optimizer=optim.SGD(params_to_update, lr=lr, momentum=mmt)\n",
        "    #optimizer = optim.SGD(params_to_update, lr=lr)\n",
        "    #optimizer = optim.Adam(params_to_update, lr = lr)\n",
        "    \n",
        "    # Listas --------------------------\n",
        "    train_correct_list = []\n",
        "    train_predict_list = []\n",
        "    \n",
        "    valid_correct_list = []\n",
        "    valid_predict_list = []\n",
        "    \n",
        "    test_correct_list = []\n",
        "    test_predict_list = []\n",
        "    #----------------------------------\n",
        "    \n",
        "    def conf_matrix(fx, y, nome):\n",
        "    \n",
        "        if(nome == 'treino'):\n",
        "          \n",
        "          preds = fx.max(1, keepdim=True)[1]\n",
        "          correct = y\n",
        "    \n",
        "          c = correct.tolist()\n",
        "          p = preds.flatten().tolist()\n",
        "          \n",
        "          train_correct_list.append(c)\n",
        "          train_predict_list.append(p)\n",
        "          \n",
        "          return train_correct_list, train_predict_list\n",
        "        \n",
        "        if(nome == 'validacao'):\n",
        "          \n",
        "          preds = fx.max(1, keepdim=True)[1]\n",
        "          correct = y\n",
        "    \n",
        "          c = correct.tolist()\n",
        "          p = preds.flatten().tolist()\n",
        "    \n",
        "          valid_correct_list.append(c)\n",
        "          valid_predict_list.append(p)\n",
        "          \n",
        "          return valid_correct_list, valid_predict_list\n",
        "    \n",
        "        if(nome == 'teste'):\n",
        "          \n",
        "          preds = fx.max(1, keepdim=True)[1]\n",
        "          correct = y\n",
        "    \n",
        "          c = correct.tolist()\n",
        "          p = preds.flatten().tolist()\n",
        "    \n",
        "          test_correct_list.append(c)\n",
        "          test_predict_list.append(p)\n",
        "    \n",
        "          return test_correct_list, test_predict_list\n",
        "      \n",
        "    def calculate_accuracy(fx, y):\n",
        "        preds = fx.max(1, keepdim=True)[1]\n",
        "        correct = preds.eq(y.view_as(preds)).sum()\n",
        "        acc = correct.float()/preds.shape[0]\n",
        "    \n",
        "        return acc\n",
        "    \n",
        "    def train(model, device, iterator, optimizer, criterion, nome):\n",
        "        \n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "    \n",
        "        model.train()\n",
        "        \n",
        "        for (x, y) in iterator:\n",
        "            \n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "                    \n",
        "            fx = model(x)\n",
        "            \n",
        "            loss = criterion(fx, y)\n",
        "            \n",
        "            acc = calculate_accuracy(fx, y)\n",
        "            \n",
        "            #Matriz de Confusão. Recebe os dados e gera listas\n",
        "            c, p = conf_matrix(fx, y, nome)\n",
        "    \n",
        "            loss.backward()\n",
        "            \n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "        return epoch_loss / len(iterator), epoch_acc / len(iterator), c, p\n",
        "    \n",
        "    def evaluate(model, device, iterator, criterion, nome):\n",
        "    \n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "        \n",
        "        model.eval()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for (x, y) in iterator:\n",
        "    \n",
        "                x = x.to(device)\n",
        "                y = y.to(device)\n",
        "    \n",
        "                fx = model(x)\n",
        "    \n",
        "                loss = criterion(fx, y)\n",
        "    \n",
        "                acc = calculate_accuracy(fx, y)\n",
        "    \n",
        "                #Matriz de Confusão. Recebe os dados e gera listas\n",
        "                c, p = conf_matrix(fx, y, nome)\n",
        "    \n",
        "                epoch_loss += loss.item()\n",
        "                epoch_acc += acc.item()\n",
        "            \n",
        "        return epoch_loss / len(iterator), epoch_acc / len(iterator), c, p\n",
        "    \n",
        "    # Cria a pasta \"./Resultados\"\n",
        "    # Cria a pasta \"./Resultados/\"model_name\"\n",
        "    \n",
        "    name = './Resultados'\n",
        "    if os.path.isdir(name) == False:   \n",
        "        os.mkdir(name)\n",
        "          \n",
        "    resultados_dir = './Resultados/'+model_name\n",
        "    if os.path.isdir(resultados_dir) == False:   \n",
        "        os.mkdir(resultados_dir)\n",
        "    \n",
        "    # Lista para calcular a media da validação e teste\n",
        "    media_val = []\n",
        "    media_test = []\n",
        "    \n",
        "    def train_function(i, x):\n",
        "      train_losses = []\n",
        "      val_losses = []\n",
        "    \n",
        "      train_accuracy = []\n",
        "      val_accuracy = []\n",
        "      \n",
        "      EPOCHS = num_epochs\n",
        "      SAVE_DIR = 'models'\n",
        "      MODEL_SAVE_PATH = os.path.join(SAVE_DIR, model_name+'-packetvision.pth')\n",
        "    \n",
        "      best_valid_loss = float('inf')\n",
        "    \n",
        "      if not os.path.isdir(f'{SAVE_DIR}'):\n",
        "          os.makedirs(f'{SAVE_DIR}')\n",
        "          \n",
        "      f = open(resultados_dir+'/'+x+'.csv', 'w', newline='') # Cria o arquivo .csv baseado no round em execução (ex: round_1.csv)\n",
        "      writer = csv.writer(f)\n",
        "      \n",
        "      \n",
        "      writer.writerow( ['Epoch', 'Train Loss', 'Train Acc', 'Val. Loss', 'Val. Acc'] )\n",
        "      \n",
        "      for epoch in range(EPOCHS):\n",
        "    \n",
        "          train_loss, train_acc, train_correct, train_pred = train(model, device, train_loaders[i], optimizer, criterion, 'treino')\n",
        "          \n",
        "          valid_loss, valid_acc, valid_correct, valid_pred = evaluate(model, device, valid_loaders[i], criterion, 'validacao')\n",
        "          \n",
        "          # Pegar somente a ultima Época para relatório da matriz de confusão\n",
        "          if(epoch != (EPOCHS-1)):\n",
        "            valid_correct.clear()\n",
        "            valid_pred.clear()\n",
        "    \n",
        "          if valid_loss < best_valid_loss:\n",
        "              best_valid_loss = valid_loss\n",
        "              torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "          \n",
        "          print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:05.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:05.2f}% |')\n",
        "          writer.writerow( [(epoch+1), (train_loss), (train_acc*100), (valid_loss), (valid_acc*100)] ) # Escreve os dados no arquivo .csv\n",
        "          \n",
        "          train_losses.append(train_loss)\n",
        "          val_losses.append(valid_loss)\n",
        "    \n",
        "          train_accuracy.append(train_acc)\n",
        "          val_accuracy.append(valid_acc)\n",
        "          \n",
        "      model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "     \n",
        "    \n",
        "      #test_loss, test_acc = evaluate(model, device, test_loader, criterion, 'teste')\n",
        "      test_loss, test_acc, test_correct, test_preds = evaluate(model, device, test_loaders[i], criterion, 'teste')\n",
        "      \n",
        "      # Matriz de Confusão para teste\n",
        "      print(\"===========================================================================================\")\n",
        "      matriz_confusao(test_correct, test_preds, 'teste', x)\n",
        "      print(\"===========================================================================================\")\n",
        "      \n",
        "    \n",
        "      print(f'| Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:05.2f}% |')\n",
        "    \n",
        "    \n",
        "      # Escreve os dados no arquivo .csv\n",
        "      writer.writerow( ['', '', '', '', ''] )\n",
        "      writer.writerow( ['Test Loss', 'Test Acc', '', '', ''] )\n",
        "      writer.writerow( [(test_loss), (test_acc*100), '', '', ''])\n",
        "      \n",
        "      # Guarda os resultados finais de validação e teste dos folds para posteriormente calcular a média\n",
        "      media_val.append(valid_acc*100)\n",
        "      media_test.append(test_acc*100)\n",
        "      \n",
        "      f.close() # Fecha arquivo .csv\n",
        "      \n",
        "    #   %matplotlib inline\n",
        "    #   %config InlineBackend.figure_format = 'retina'\n",
        "    \n",
        "      # Cria os graficos de decaimento treino e validação (imprime na tela e salva na pasta \"./Resultados\")\n",
        "      plt.title(x)\n",
        "      plt.plot(train_losses, label='Loss')\n",
        "      #plt.plot(val_losses, label='Validation loss')\n",
        "      plt.plot(train_accuracy, label='Accuracy')\n",
        "      plt.legend(frameon=False)\n",
        "      plt.grid()\n",
        "      plt.savefig(resultados_dir+'/'+'graf_'+x+'.png')\n",
        "      plt.close()\n",
        "      #plt.show()\n",
        "      '''\n",
        "      plt.title(x)\n",
        "      plt.plot(train_losses, label='Training loss')\n",
        "      plt.plot(val_losses, label='Validation loss')\n",
        "      plt.legend(frameon=False)\n",
        "      plt.savefig(resultados_dir+'/'+'graf_'+x+'.png')\n",
        "      plt.close()\n",
        "      #plt.show()\n",
        "    \n",
        "      # Cria os graficos de acurácia do treino e validação (imprime na tela e salva na pasta \"./Resultados\")\n",
        "      plt.title(x)\n",
        "      plt.plot(train_accuracy, label='Training accuracy')\n",
        "      plt.plot(val_accuracy, label='Validation accuracy')\n",
        "      plt.legend(frameon=False)\n",
        "      plt.savefig(resultados_dir+'/'+'grafAcc_'+x+'.png')\n",
        "      plt.close()\n",
        "      #plt.show()\n",
        "      '''\n",
        "    def clear_list():\n",
        "      train_correct_list.clear()\n",
        "      train_predict_list.clear()\n",
        "      \n",
        "      valid_correct_list.clear()\n",
        "      valid_predict_list.clear()\n",
        "    \n",
        "      test_correct_list.clear()\n",
        "      test_predict_list.clear()\n",
        "    \n",
        "    # Matriz de Confusão\n",
        "    def matriz_confusao(correct, pred, nome, x):\n",
        "      correct_list = []\n",
        "      predict_list = []\n",
        "      \n",
        "      for i in correct:\n",
        "          correct_list.extend(i)\n",
        "    \n",
        "      for j in pred:\n",
        "          predict_list.extend(j)\n",
        "          \n",
        "      \n",
        "      print(\"Listas: \")\n",
        "      print(\"Correct: \", correct_list)\n",
        "      print(\"Predict: \", predict_list)\n",
        "      print(\"\\n\")\n",
        "    \n",
        "      print(\"Matriz de Confusão (\"+nome+\") do \"+x+\": \")\n",
        "      cm = confusion_matrix(correct_list, predict_list)\n",
        "      print(cm)\n",
        "      \n",
        "      ax = plt.subplot()\n",
        "      sns.heatmap(cm, annot=True, cmap=\"YlGnBu\", fmt=\"d\") #annot=True to annotate cells\n",
        "      ax.set_title('Matriz de Confusão ('+nome+') ('+x+')')\n",
        "      plt.savefig(resultados_dir+'/'+'matrizconfusao_'+nome[:3]+'_'+x+'.png')\n",
        "      plt.close() \n",
        "      plt.show()\n",
        "    \n",
        "      print(\"\\nRelatório de classificação (\"+nome+\"): \")\n",
        "      report = metrics.classification_report(correct_list, predict_list, target_names=classes, digits =4)\n",
        "      print(metrics.classification_report(correct_list, predict_list, target_names=classes, digits =4))\n",
        "    \n",
        "      file_report = open('./Resultados/'+model_name+'/report_matrix.txt', 'a+')\n",
        "      file_report.write(\"%s \\n\" %nome)\n",
        "      file_report.write(\"%s \\n\" %x)\n",
        "      file_report.write(report)\n",
        "      file_report.write(\"\\n\")\n",
        "      file_report.write(\"Matriz de Confusão \\n\")\n",
        "      file_report.write(str(cm))\n",
        "      file_report.write(\"\\n \\n\")\n",
        "      file_report.close()\n",
        "      \n",
        "      '''\n",
        "       # calculate the fpr and tpr for all thresholds of the classification\n",
        "      fpr, tpr, threshold = metrics.roc_curve(correct_list, predict_list)\n",
        "      roc_auc = metrics.auc(fpr, tpr)\n",
        "    \n",
        "      # method I: plt\n",
        "      plt.title('Receiver Operating Characteristic')\n",
        "      plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "      plt.legend(loc = 'lower right')\n",
        "      plt.plot([0, 1], [0, 1],'r--')\n",
        "      plt.xlim([0, 1])\n",
        "      plt.ylim([0, 1])\n",
        "      plt.ylabel('True Positive Rate')\n",
        "      plt.xlabel('False Positive Rate')\n",
        "      plt.savefig(resultados_dir+'/'+'ROC_'+nome[:3]+'_'+x+'.png')\n",
        "      plt.close() \n",
        "      #plt.show()\n",
        "      '''    \n",
        "    \n",
        "    def calc_time(time_total):\n",
        "      segundos = time_total\n",
        "    \n",
        "      segundos_rest = segundos % 86400\n",
        "      horas = segundos_rest // 3600\n",
        "      segundos_rest = segundos_rest % 3600\n",
        "      minutos = segundos_rest // 60\n",
        "      segundos_rest = segundos_rest % 60\n",
        "    \n",
        "      plot_time = \"%d hours\" %horas+\", %d minutes\" %minutos+\", %d seconds\" %segundos_rest\n",
        "      return plot_time\n",
        "    \n",
        "    def weights_init(m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            torch.nn.init.xavier_uniform(m.weight.data)\n",
        "            torch.nn.init.xavier_uniform(m.bias.data)\n",
        "    \n",
        "    # Executando a função de treino para cada fold e calcula média (validação e treino)\n",
        "    \n",
        "    for i in range(len(folds)):\n",
        "      print(\"===========================================================================================\")\n",
        "      print(\"Inicialização do Particionamento, \", folds[i])\n",
        "      print(\"===========================================================================================\")\n",
        "      \n",
        "      clear_list() # A cada execução do laço de repetição as listas que coletam as preds e corrects são limpadas\n",
        "      start = time.time()\n",
        "      train_function(i, str(folds[i]))\n",
        "      end = time.time()\n",
        "      time_total = (end-start)\n",
        "      \n",
        "      plot_time = calc_time(time_total)\n",
        "      print(plot_time)\n",
        "      \n",
        "      tempo_txt = open('./Resultados/'+model_name+'/tempo_treinamento.txt', 'a+')\n",
        "      tempo_txt.write(folds[i]+\": \")\n",
        "      tempo_txt.write(plot_time+\"\\n\")\n",
        "      tempo_txt.close()\n",
        "      print(\"===========================================================================================\")\n",
        "      print(\"\\n\")\n",
        "    \n",
        "    # Média do k-fold\n",
        "    print(media_val)\n",
        "    kfold = 0\n",
        "    for i in range(len(media_val)):\n",
        "      kfold = kfold + media_val[i]\n",
        "      \n",
        "    media_kfold_val = kfold/(len(media_val))\n",
        "    \n",
        "    print('\\nVal Accuracy (Median): %.2f%%' % (media_kfold_val))\n",
        "    \n",
        "    print(media_test)\n",
        "    kfold = 0\n",
        "    for i in range(len(media_test)):\n",
        "      kfold = kfold + media_test[i]\n",
        "      \n",
        "    media_kfold_test = kfold/(len(media_test))\n",
        "    \n",
        "    print('\\nTest Accuracy (Median): %.2f%%' % (media_kfold_test))\n",
        "    \n",
        "    m = open(resultados_dir+'/'+'medias.csv', 'w', newline='')\n",
        "    writer = csv.writer(m)\n",
        "    \n",
        "    writer.writerow( ['Val Accuracy', 'Test Accuracy'] )\n",
        "    writer.writerow( [(media_kfold_val), (media_kfold_test)] )\n",
        "    m.close()\n",
        "    \n",
        "    # Limpa listas para possíveis novas execuções em células\n",
        "    media_val.clear()\n",
        "    media_test.clear()\n",
        "   \n",
        "        \n",
        "        \n",
        "    \n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Dev/VINEVI/dataset/5-fold/round_1/train/\n",
            "/content/gdrive/MyDrive/Dev/VINEVI/dataset/5-fold/round_1/test/\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f63c92b2390>\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f63c8b0f690>\n",
            "----------------------------------------------------------------------------------------\n",
            "round_1\n",
            "Num training images:  7713 <torch.utils.data.dataloader.DataLoader object at 0x7f63c92b2390>\n",
            "Num valid images:  962 <torch.utils.data.dataloader.DataLoader object at 0x7f63ca2094d0>\n",
            "Num test images:  970 <torch.utils.data.dataloader.DataLoader object at 0x7f63c8b0f690>\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------\n",
            "Num train full size: 7713\n",
            "Num valid full size: 962\n",
            "Num test images:  970 <torch.utils.data.dataloader.DataLoader object at 0x7f63c8b0f690>\n",
            "Num full size (train+valid): 9637\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len: 2\n",
            "types: <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
            "shapes: torch.Size([32, 3, 224, 224]) torch.Size([32])\n",
            "torch.Size([3, 224, 224])\n",
            "tensor(0)\n",
            "labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Treinamento em GPU!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/squeezenet1_0-b66bff10.pth\" to /root/.cache/torch/hub/checkpoints/squeezenet1_0-b66bff10.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "120f5be2f7f648fb9cf67555394fc929",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/4.78M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params to learn:\n",
            "\t features.0.weight\n",
            "\t features.0.bias\n",
            "\t features.3.squeeze.weight\n",
            "\t features.3.squeeze.bias\n",
            "\t features.3.expand1x1.weight\n",
            "\t features.3.expand1x1.bias\n",
            "\t features.3.expand3x3.weight\n",
            "\t features.3.expand3x3.bias\n",
            "\t features.4.squeeze.weight\n",
            "\t features.4.squeeze.bias\n",
            "\t features.4.expand1x1.weight\n",
            "\t features.4.expand1x1.bias\n",
            "\t features.4.expand3x3.weight\n",
            "\t features.4.expand3x3.bias\n",
            "\t features.5.squeeze.weight\n",
            "\t features.5.squeeze.bias\n",
            "\t features.5.expand1x1.weight\n",
            "\t features.5.expand1x1.bias\n",
            "\t features.5.expand3x3.weight\n",
            "\t features.5.expand3x3.bias\n",
            "\t features.7.squeeze.weight\n",
            "\t features.7.squeeze.bias\n",
            "\t features.7.expand1x1.weight\n",
            "\t features.7.expand1x1.bias\n",
            "\t features.7.expand3x3.weight\n",
            "\t features.7.expand3x3.bias\n",
            "\t features.8.squeeze.weight\n",
            "\t features.8.squeeze.bias\n",
            "\t features.8.expand1x1.weight\n",
            "\t features.8.expand1x1.bias\n",
            "\t features.8.expand3x3.weight\n",
            "\t features.8.expand3x3.bias\n",
            "\t features.9.squeeze.weight\n",
            "\t features.9.squeeze.bias\n",
            "\t features.9.expand1x1.weight\n",
            "\t features.9.expand1x1.bias\n",
            "\t features.9.expand3x3.weight\n",
            "\t features.9.expand3x3.bias\n",
            "\t features.10.squeeze.weight\n",
            "\t features.10.squeeze.bias\n",
            "\t features.10.expand1x1.weight\n",
            "\t features.10.expand1x1.bias\n",
            "\t features.10.expand3x3.weight\n",
            "\t features.10.expand3x3.bias\n",
            "\t features.12.squeeze.weight\n",
            "\t features.12.squeeze.bias\n",
            "\t features.12.expand1x1.weight\n",
            "\t features.12.expand1x1.bias\n",
            "\t features.12.expand3x3.weight\n",
            "\t features.12.expand3x3.bias\n",
            "\t classifier.1.weight\n",
            "\t classifier.1.bias\n",
            "===========================================================================================\n",
            "Inicialização do Particionamento,  round_1\n",
            "===========================================================================================\n",
            "| Epoch: 01 | Train Loss: 0.538 | Train Acc: 79.47% | Val. Loss: 0.258 | Val. Acc: 88.51% |\n",
            "| Epoch: 02 | Train Loss: 0.143 | Train Acc: 95.21% | Val. Loss: 0.081 | Val. Acc: 96.57% |\n",
            "| Epoch: 03 | Train Loss: 0.097 | Train Acc: 96.66% | Val. Loss: 0.063 | Val. Acc: 98.29% |\n",
            "| Epoch: 04 | Train Loss: 0.073 | Train Acc: 97.48% | Val. Loss: 0.092 | Val. Acc: 96.57% |\n",
            "| Epoch: 05 | Train Loss: 0.056 | Train Acc: 98.06% | Val. Loss: 0.073 | Val. Acc: 97.58% |\n",
            "| Epoch: 06 | Train Loss: 0.049 | Train Acc: 98.21% | Val. Loss: 0.033 | Val. Acc: 98.89% |\n",
            "| Epoch: 07 | Train Loss: 0.041 | Train Acc: 98.48% | Val. Loss: 0.083 | Val. Acc: 96.37% |\n",
            "| Epoch: 08 | Train Loss: 0.042 | Train Acc: 98.59% | Val. Loss: 0.036 | Val. Acc: 98.69% |\n",
            "| Epoch: 09 | Train Loss: 0.034 | Train Acc: 98.70% | Val. Loss: 0.012 | Val. Acc: 99.60% |\n",
            "| Epoch: 10 | Train Loss: 0.029 | Train Acc: 99.02% | Val. Loss: 0.142 | Val. Acc: 94.25% |\n",
            "| Epoch: 11 | Train Loss: 0.042 | Train Acc: 98.73% | Val. Loss: 0.024 | Val. Acc: 98.69% |\n",
            "| Epoch: 12 | Train Loss: 0.025 | Train Acc: 99.01% | Val. Loss: 0.024 | Val. Acc: 98.89% |\n",
            "| Epoch: 13 | Train Loss: 0.019 | Train Acc: 99.33% | Val. Loss: 0.027 | Val. Acc: 98.29% |\n",
            "| Epoch: 14 | Train Loss: 0.016 | Train Acc: 99.46% | Val. Loss: 0.029 | Val. Acc: 98.39% |\n",
            "| Epoch: 15 | Train Loss: 0.015 | Train Acc: 99.51% | Val. Loss: 0.034 | Val. Acc: 98.79% |\n",
            "| Epoch: 16 | Train Loss: 0.020 | Train Acc: 99.21% | Val. Loss: 0.013 | Val. Acc: 99.40% |\n",
            "| Epoch: 17 | Train Loss: 0.015 | Train Acc: 99.38% | Val. Loss: 0.030 | Val. Acc: 98.39% |\n",
            "| Epoch: 18 | Train Loss: 0.014 | Train Acc: 99.38% | Val. Loss: 0.016 | Val. Acc: 99.09% |\n",
            "| Epoch: 19 | Train Loss: 0.013 | Train Acc: 99.57% | Val. Loss: 0.016 | Val. Acc: 99.70% |\n",
            "| Epoch: 20 | Train Loss: 0.013 | Train Acc: 99.48% | Val. Loss: 0.035 | Val. Acc: 98.89% |\n",
            "| Epoch: 21 | Train Loss: 0.010 | Train Acc: 99.61% | Val. Loss: 0.009 | Val. Acc: 99.80% |\n",
            "| Epoch: 22 | Train Loss: 0.011 | Train Acc: 99.57% | Val. Loss: 0.011 | Val. Acc: 99.70% |\n",
            "| Epoch: 23 | Train Loss: 0.018 | Train Acc: 99.34% | Val. Loss: 0.017 | Val. Acc: 99.40% |\n",
            "| Epoch: 24 | Train Loss: 0.014 | Train Acc: 99.51% | Val. Loss: 0.021 | Val. Acc: 99.29% |\n",
            "| Epoch: 25 | Train Loss: 0.007 | Train Acc: 99.77% | Val. Loss: 0.011 | Val. Acc: 99.60% |\n",
            "| Epoch: 26 | Train Loss: 0.010 | Train Acc: 99.63% | Val. Loss: 0.017 | Val. Acc: 99.50% |\n",
            "| Epoch: 27 | Train Loss: 0.008 | Train Acc: 99.66% | Val. Loss: 0.016 | Val. Acc: 99.40% |\n",
            "| Epoch: 28 | Train Loss: 0.013 | Train Acc: 99.50% | Val. Loss: 0.018 | Val. Acc: 99.50% |\n",
            "| Epoch: 29 | Train Loss: 0.013 | Train Acc: 99.54% | Val. Loss: 0.011 | Val. Acc: 99.60% |\n",
            "| Epoch: 30 | Train Loss: 0.006 | Train Acc: 99.75% | Val. Loss: 0.006 | Val. Acc: 99.70% |\n",
            "| Epoch: 31 | Train Loss: 0.008 | Train Acc: 99.73% | Val. Loss: 0.021 | Val. Acc: 99.29% |\n",
            "| Epoch: 32 | Train Loss: 0.008 | Train Acc: 99.73% | Val. Loss: 0.007 | Val. Acc: 99.70% |\n",
            "| Epoch: 33 | Train Loss: 0.008 | Train Acc: 99.66% | Val. Loss: 0.011 | Val. Acc: 99.60% |\n",
            "| Epoch: 34 | Train Loss: 0.006 | Train Acc: 99.72% | Val. Loss: 0.009 | Val. Acc: 99.60% |\n",
            "| Epoch: 35 | Train Loss: 0.008 | Train Acc: 99.65% | Val. Loss: 0.007 | Val. Acc: 99.80% |\n",
            "| Epoch: 36 | Train Loss: 0.006 | Train Acc: 99.74% | Val. Loss: 0.006 | Val. Acc: 99.80% |\n",
            "| Epoch: 37 | Train Loss: 0.013 | Train Acc: 99.50% | Val. Loss: 0.009 | Val. Acc: 99.70% |\n",
            "| Epoch: 38 | Train Loss: 0.010 | Train Acc: 99.65% | Val. Loss: 0.011 | Val. Acc: 99.70% |\n",
            "| Epoch: 39 | Train Loss: 0.004 | Train Acc: 99.87% | Val. Loss: 0.009 | Val. Acc: 99.80% |\n",
            "| Epoch: 40 | Train Loss: 0.012 | Train Acc: 99.55% | Val. Loss: 0.008 | Val. Acc: 99.80% |\n",
            "| Epoch: 41 | Train Loss: 0.007 | Train Acc: 99.75% | Val. Loss: 0.012 | Val. Acc: 99.40% |\n",
            "| Epoch: 42 | Train Loss: 0.013 | Train Acc: 99.60% | Val. Loss: 0.006 | Val. Acc: 99.80% |\n",
            "| Epoch: 43 | Train Loss: 0.010 | Train Acc: 99.63% | Val. Loss: 0.009 | Val. Acc: 99.60% |\n",
            "| Epoch: 44 | Train Loss: 0.013 | Train Acc: 99.61% | Val. Loss: 0.007 | Val. Acc: 99.70% |\n",
            "| Epoch: 45 | Train Loss: 0.005 | Train Acc: 99.83% | Val. Loss: 0.007 | Val. Acc: 99.70% |\n",
            "| Epoch: 46 | Train Loss: 0.006 | Train Acc: 99.74% | Val. Loss: 0.011 | Val. Acc: 99.70% |\n",
            "| Epoch: 47 | Train Loss: 0.005 | Train Acc: 99.82% | Val. Loss: 0.004 | Val. Acc: 99.80% |\n",
            "| Epoch: 48 | Train Loss: 0.010 | Train Acc: 99.69% | Val. Loss: 0.018 | Val. Acc: 98.99% |\n",
            "| Epoch: 49 | Train Loss: 0.012 | Train Acc: 99.56% | Val. Loss: 0.031 | Val. Acc: 98.59% |\n",
            "| Epoch: 50 | Train Loss: 0.006 | Train Acc: 99.77% | Val. Loss: 0.017 | Val. Acc: 99.40% |\n",
            "===========================================================================================\n",
            "Listas: \n",
            "Correct:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
            "Predict:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
            "\n",
            "\n",
            "Matriz de Confusão (teste) do round_1: \n",
            "[[120   0   0   3   0   0   0]\n",
            " [  0 123   0   0   0   0   0]\n",
            " [  0   0 142   0   0   0   0]\n",
            " [  2   0   0 184   0   0   0]\n",
            " [  0   0   0   0 128   0   0]\n",
            " [  0   0   0   0   0 136   0]\n",
            " [  0   0   0   0   0   0 132]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:617: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Relatório de classificação (teste): \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  bittorrent     0.9836    0.9756    0.9796       123\n",
            "    browsing     1.0000    1.0000    1.0000       123\n",
            "         dns     1.0000    1.0000    1.0000       142\n",
            "         iot     0.9840    0.9892    0.9866       186\n",
            "         rdp     1.0000    1.0000    1.0000       128\n",
            "         ssh     1.0000    1.0000    1.0000       136\n",
            "        voip     1.0000    1.0000    1.0000       132\n",
            "\n",
            "    accuracy                         0.9948       970\n",
            "   macro avg     0.9954    0.9950    0.9952       970\n",
            "weighted avg     0.9948    0.9948    0.9948       970\n",
            "\n",
            "===========================================================================================\n",
            "| Test Loss: 0.016 | Test Acc: 99.50% |\n",
            "1 hours, 10 minutes, 33 seconds\n",
            "===========================================================================================\n",
            "\n",
            "\n",
            "[99.39516129032258]\n",
            "\n",
            "Val Accuracy (Median): 99.40%\n",
            "[99.49596774193549]\n",
            "\n",
            "Test Accuracy (Median): 99.50%\n"
          ]
        }
      ]
    }
  ]
}